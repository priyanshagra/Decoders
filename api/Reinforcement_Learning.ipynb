{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4b400ac",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a470ac",
   "metadata": {},
   "source": [
    "# Single Agent and Enviroment without Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b66325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class TrainSchedulingEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(TrainSchedulingEnv, self).__init__()\n",
    "\n",
    "        # Stations and their routes (in minutes)\n",
    "        self.stations = [\"Chennai\", \"Madurai\", \"Coimbatore\", \"Tiruchirappalli\", \"Salem\"]\n",
    "        self.routes = {\n",
    "            \"Chennai\": {\"Madurai\": 420, \"Coimbatore\": 480, \"Tiruchirappalli\": 330},\n",
    "            \"Madurai\": {\"Chennai\": 420, \"Tiruchirappalli\": 120, \"Coimbatore\": 240},\n",
    "            \"Coimbatore\": {\"Chennai\": 480, \"Madurai\": 240, \"Salem\": 210},\n",
    "            \"Tiruchirappalli\": {\"Chennai\": 330, \"Madurai\": 120, \"Salem\": 150},\n",
    "            \"Salem\": {\"Coimbatore\": 210, \"Tiruchirappalli\": 150}\n",
    "        }\n",
    "        self.num_stations = len(self.stations)\n",
    "\n",
    "        # Train details\n",
    "        self.num_trains = 5\n",
    "        self.max_tracks_per_station = 3\n",
    "\n",
    "        # Weather and maintenance\n",
    "        self.weather_conditions = {route: np.random.uniform(0.8, 1.2) for route in self.routes.keys()}\n",
    "        self.maintenance_schedule = {route: np.random.choice([0, 1], p=[0.9, 0.1]) for route in self.routes.keys()}  # 10% chance of maintenance\n",
    "\n",
    "        # Passenger demand (randomized at reset)\n",
    "        self.passenger_demand = None\n",
    "\n",
    "        # Action and state spaces\n",
    "        self.action_space = spaces.MultiDiscrete([self.num_stations] * self.num_trains)  # Next station for each train\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(self.num_trains, 6), dtype=np.float32\n",
    "        )  # [current_station, delay, destination, progress, weather, maintenance]\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.train_states = np.zeros((self.num_trains, 6))\n",
    "        self.passenger_demand = np.random.randint(50, 200, size=(self.num_stations, self.num_stations))\n",
    "        for train_id in range(self.num_trains):\n",
    "            self.train_states[train_id, 0] = np.random.randint(0, self.num_stations)  # Random starting station\n",
    "            self.train_states[train_id, 2] = np.random.randint(0, self.num_stations)  # Random destination\n",
    "        return self.train_states\n",
    "\n",
    "    def step(self, actions):\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        for train_id, next_station_index in enumerate(actions):\n",
    "            current_station_index = int(self.train_states[train_id, 0])\n",
    "            destination_station_index = int(self.train_states[train_id, 2])\n",
    "            current_station = self.stations[current_station_index]\n",
    "            next_station = self.stations[next_station_index]\n",
    "\n",
    "            # Check if route exists\n",
    "            if next_station not in self.routes[current_station]:\n",
    "                reward -= 50  # Heavy penalty for invalid route choice\n",
    "                continue\n",
    "\n",
    "            # Travel time, weather, and maintenance impact\n",
    "            travel_time = self.routes[current_station][next_station]\n",
    "            weather_impact = self.weather_conditions[current_station]\n",
    "            maintenance = self.maintenance_schedule[current_station]\n",
    "\n",
    "            if maintenance:\n",
    "                reward -= 30  # Penalty for choosing a route under maintenance\n",
    "                continue\n",
    "\n",
    "            adjusted_travel_time = travel_time * weather_impact\n",
    "            self.train_states[train_id, 1] += adjusted_travel_time  # Add delay\n",
    "            self.train_states[train_id, 0] = next_station_index  # Update current station\n",
    "\n",
    "            # Reward based on passenger satisfaction\n",
    "            passengers_satisfied = self.passenger_demand[current_station_index, destination_station_index]\n",
    "            reward += passengers_satisfied * 2 - adjusted_travel_time  # Positive reward for satisfying demand and minimizing delay\n",
    "\n",
    "            # Check if destination reached\n",
    "            if current_station_index == destination_station_index:\n",
    "                reward += 100  # Large reward for reaching destination\n",
    "                done = True  # Simulation ends when a train reaches its destination\n",
    "\n",
    "        return self.train_states, reward, done, {}\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        print(f\"Train States: {self.train_states}\")\n",
    "        for train_id, train_state in enumerate(self.train_states):\n",
    "            current_station = self.stations[int(train_state[0])]\n",
    "            destination_station = self.stations[int(train_state[2])]\n",
    "            print(\n",
    "                f\"Train {train_id}: Current Station={current_station}, Destination={destination_station}, \"\n",
    "                f\"Delay={train_state[1]:.2f} minutes\"\n",
    "            )\n",
    "\n",
    "# Create environment\n",
    "env = TrainSchedulingEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89afacf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Wrap the environment\n",
    "env = make_vec_env(lambda: TrainSchedulingEnv(), n_envs=1)\n",
    "\n",
    "# Train the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=50000)\n",
    "\n",
    "# Test the model\n",
    "obs = env.reset()\n",
    "for _ in range(100):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    env.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43a9500",
   "metadata": {},
   "source": [
    "## Even tried to simulate realworld train paths of india to check whether this solution works but it fails "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b42c1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import time  # To add delay for visualization\n",
    "\n",
    "# Load the dataset\n",
    "train_data = pd.read_csv('Train_details_22122017.csv', dtype={'Distance': str})\n",
    "\n",
    "# Convert the 'Distance' column to numeric, coercing errors into NaN\n",
    "train_data['Distance'] = pd.to_numeric(train_data['Distance'], errors='coerce')\n",
    "\n",
    "# Select data for multiple trains (e.g., Train No 107, 108, 109)\n",
    "train_numbers = [107, 108, 109]\n",
    "train_subset = train_data\n",
    "\n",
    "# Create a graph with NetworkX\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add stations as nodes\n",
    "for _, row in train_subset.iterrows():\n",
    "    G.add_node(row[\"Station Name\"], station_code=row[\"Station Code\"])\n",
    "\n",
    "# Add routes (edges) between stations for each train\n",
    "for i in range(1, len(train_subset)):\n",
    "    source = train_subset.iloc[i-1]\n",
    "    target = train_subset.iloc[i]\n",
    "\n",
    "    if pd.notna(source[\"Distance\"]) and pd.notna(target[\"Distance\"]):\n",
    "        G.add_edge(\n",
    "            source[\"Station Name\"],\n",
    "            target[\"Station Name\"],\n",
    "            distance=target[\"Distance\"] - source[\"Distance\"],\n",
    "            departure_time=target[\"Departure Time\"],\n",
    "            arrival_time=source[\"Arrival time\"]\n",
    "        )\n",
    "\n",
    "# Check the graph\n",
    "print(f\"Number of nodes (stations) in the graph: {len(G.nodes())}\")\n",
    "print(f\"Number of edges (routes) in the graph: {len(G.edges())}\")\n",
    "\n",
    "# Ensure that there are nodes to create the action space\n",
    "if len(G.nodes()) == 0:\n",
    "    raise ValueError(\"Graph has no nodes. Ensure the dataset contains valid station data.\")\n",
    "\n",
    "# Define the environment\n",
    "class TrainMovementEnv(gym.Env):\n",
    "    def __init__(self, graph):\n",
    "        super(TrainMovementEnv, self).__init__()\n",
    "        self.graph = graph\n",
    "        self.stations = list(graph.nodes())  # List of station names\n",
    "        self.current_station = 0  # Start at the first station\n",
    "        self.action_space = spaces.Discrete(len(self.stations))  # Actions: move to a station\n",
    "        self.observation_space = spaces.Discrete(len(self.stations))  # Observations: current station\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_station = 0  # Reset to the first station\n",
    "        return self.current_station\n",
    "\n",
    "    def step(self, action):\n",
    "        if action < 0 or action >= len(self.stations):\n",
    "            return self.current_station, -1, False, {}\n",
    "\n",
    "        # Move to the selected station\n",
    "        next_station = self.stations[action]\n",
    "        if next_station in self.graph[self.stations[self.current_station]]:\n",
    "            self.current_station = action\n",
    "            return self.current_station, 0, False, {}  # No reward, still moving\n",
    "        else:\n",
    "            return self.current_station, -1, False, {}  # Invalid move\n",
    "\n",
    "    def render(self):\n",
    "        # Plot the graph with the current station highlighted\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        pos = nx.spring_layout(self.graph, seed=42)  # Use a layout for better positioning\n",
    "        nx.draw(self.graph, pos, with_labels=True, node_size=5000, node_color='skyblue', font_size=10)\n",
    "\n",
    "        # Highlight the current station in a different color\n",
    "        current_station = self.stations[self.current_station]\n",
    "        nx.draw_networkx_nodes(self.graph, pos, nodelist=[current_station], node_size=5000, node_color='red')\n",
    "\n",
    "        plt.title(f\"Train is at {current_station}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Adding a small delay to visualize the movement\n",
    "        time.sleep(1)\n",
    "\n",
    "# Initialize environment with the graph of train stations\n",
    "env = TrainMovementEnv(G)\n",
    "\n",
    "# Example of simulating one episode\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()  # Choose a random action (move to a random station)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()  # Render the current state after each step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f0edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "\n",
    "# Example: Create a simple graph\n",
    "G = nx.Graph()\n",
    "G.add_edge(\"Mumbai\", \"Delhi\", weight=1000)\n",
    "G.add_edge(\"Delhi\", \"Kolkata\", weight=1500)\n",
    "G.add_edge(\"Mumbai\", \"Chennai\", weight=1300)\n",
    "\n",
    "# Plot with NetworkX\n",
    "pos = nx.spring_layout(G)  # You can replace with geospatial coordinates\n",
    "nx.draw(G, pos, with_labels=True, node_color='lightblue', edge_color='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6709dfe6",
   "metadata": {},
   "source": [
    "# Mutli Agent and Enviroment with Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63443ad7",
   "metadata": {},
   "source": [
    "## First deliverable, Reinforcement learning model for train scheduling optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da28e7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class RailEnvironment:\n",
    "    def __init__(self, grid_size=(10, 10), n_agents=3):\n",
    "        self.grid_size = grid_size\n",
    "        self.n_agents = n_agents\n",
    "        self.grid = np.full(grid_size, -1)  # Initialize all cells as non-rail area (-1)\n",
    "        self.agents = []\n",
    "        self._initialize_environment()\n",
    "\n",
    "    def _initialize_environment(self):\n",
    "        # Create railway paths (values >= 0) randomly\n",
    "        num_paths = random.randint(10, 20)\n",
    "        for _ in range(num_paths):\n",
    "            x, y = np.random.randint(0, self.grid_size[0]), np.random.randint(0, self.grid_size[1])\n",
    "            self.grid[x, y] = 0  # Mark railway paths as >= 0\n",
    "\n",
    "        # Assign random areas for maintenance (2) and extreme weather (3)\n",
    "        num_maintenance = random.randint(2, 5)\n",
    "        num_weather = random.randint(2, 5)\n",
    "\n",
    "        for _ in range(num_maintenance):\n",
    "            x, y = np.random.randint(0, self.grid_size[0]), np.random.randint(0, self.grid_size[1])\n",
    "            if self.grid[x, y] >= 0:  # Ensure it is part of a railway path\n",
    "                self.grid[x, y] = 2\n",
    "\n",
    "        for _ in range(num_weather):\n",
    "            x, y = np.random.randint(0, self.grid_size[0]), np.random.randint(0, self.grid_size[1])\n",
    "            if self.grid[x, y] >= 0:  # Ensure it is part of a railway path\n",
    "                self.grid[x, y] = 3\n",
    "\n",
    "        # Place agents (trains) at random railway stations\n",
    "        for i in range(self.n_agents):\n",
    "            start = self._get_random_railway_position()\n",
    "            target = self._get_random_railway_position()\n",
    "            while target == start:\n",
    "                target = self._get_random_railway_position()\n",
    "            self.agents.append({\n",
    "                \"id\": i,\n",
    "                \"start\": start,\n",
    "                \"target\": target,\n",
    "                \"position\": start,\n",
    "                \"done\": False,\n",
    "                \"reward\": 0\n",
    "            })\n",
    "            self.grid[start] = 1  # Mark the agent's initial position as occupied\n",
    "\n",
    "    def _get_random_railway_position(self):\n",
    "        while True:\n",
    "            x, y = np.random.randint(0, self.grid_size[0]), np.random.randint(0, self.grid_size[1])\n",
    "            if self.grid[x, y] >= 0:  # Ensure the position is part of a railway path\n",
    "                return (x, y)\n",
    "\n",
    "    def step(self, actions):\n",
    "        rewards = {}\n",
    "        done = True\n",
    "        for agent, action in zip(self.agents, actions):\n",
    "            if agent[\"done\"]:\n",
    "                rewards[agent[\"id\"]] = 0\n",
    "                continue\n",
    "\n",
    "            # Compute new position\n",
    "            new_position = self._move(agent[\"position\"], action)\n",
    "            if not self._is_valid(new_position):\n",
    "                rewards[agent[\"id\"]] = -5  # Invalid move penalty\n",
    "                continue\n",
    "\n",
    "            # Update position and check for rewards\n",
    "            current_cell_value = self.grid[new_position]\n",
    "            self.grid[agent[\"position\"]] = 0  # Mark current position as free\n",
    "            agent[\"position\"] = new_position\n",
    "            self.grid[new_position] = 1  # Mark new position as occupied\n",
    "\n",
    "            if new_position == agent[\"target\"]:\n",
    "                agent[\"done\"] = True\n",
    "                rewards[agent[\"id\"]] = 10  # Reaching target reward\n",
    "            elif current_cell_value == 3:\n",
    "                rewards[agent[\"id\"]] = -2  # Extreme weather condition penalty\n",
    "            elif current_cell_value == 2:\n",
    "                rewards[agent[\"id\"]] = -3  # Maintenance area penalty\n",
    "            else:\n",
    "                rewards[agent[\"id\"]] = -1  # Normal step penalty\n",
    "\n",
    "            done = done and agent[\"done\"]\n",
    "\n",
    "        return self.grid, rewards, done\n",
    "\n",
    "    def _move(self, position, action):\n",
    "        moves = {\n",
    "            0: (-1, 0),  # Up\n",
    "            1: (1, 0),   # Down\n",
    "            2: (0, -1),  # Left\n",
    "            3: (0, 1),   # Right\n",
    "            4: (0, 0)    # Wait\n",
    "        }\n",
    "        return (position[0] + moves[action][0], position[1] + moves[action][1])\n",
    "\n",
    "    def _is_valid(self, position):\n",
    "        return (0 <= position[0] < self.grid_size[0] and\n",
    "                0 <= position[1] < self.grid_size[1] and\n",
    "                self.grid[position] >= 0 and  # Must be part of the railway path\n",
    "                self.grid[position] != 1)    # Must not be occupied\n",
    "\n",
    "    def reset(self):\n",
    "        self.__init__(self.grid_size, self.n_agents)\n",
    "        return self.grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b57d06f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01129fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQLAgent:\n",
    "    def __init__(self, state_size, action_size, learning_rate=0.001, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.model = DQN(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.action_size)  # Random action\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state)\n",
    "        return torch.argmax(q_values).item()  # Exploitation\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += self.gamma * torch.max(self.model(next_state)).item()\n",
    "            target_f = self.model(state).detach()\n",
    "            target_f[0][action] = target\n",
    "            self.model.zero_grad()\n",
    "            loss = nn.functional.mse_loss(self.model(state), target_f)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b2258b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/1000, Total Reward: -10, Average Reward per Agent: -2.00\n",
      "Episode 2/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 3/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 4/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 5/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 6/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 7/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 8/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 9/1000, Total Reward: -47, Average Reward per Agent: -9.40\n",
      "Episode 10/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 11/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 12/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 13/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 14/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 15/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 16/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 17/1000, Total Reward: -88, Average Reward per Agent: -17.60\n",
      "Episode 18/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 19/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 20/1000, Total Reward: -84, Average Reward per Agent: -16.80\n",
      "Episode 21/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 22/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 23/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 24/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 25/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 26/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 27/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 28/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 29/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 30/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 31/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 32/1000, Total Reward: -10, Average Reward per Agent: -2.00\n",
      "Episode 33/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 34/1000, Total Reward: -88, Average Reward per Agent: -17.60\n",
      "Episode 35/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 36/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 37/1000, Total Reward: -67, Average Reward per Agent: -13.40\n",
      "Episode 38/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 39/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 40/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 41/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 42/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 43/1000, Total Reward: -67, Average Reward per Agent: -13.40\n",
      "Episode 44/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 45/1000, Total Reward: -42, Average Reward per Agent: -8.40\n",
      "Episode 46/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 47/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 48/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 49/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 50/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 51/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 52/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 53/1000, Total Reward: -67, Average Reward per Agent: -13.40\n",
      "Episode 54/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 55/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 56/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 57/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 58/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 59/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 60/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 61/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 62/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 63/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 64/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 65/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 66/1000, Total Reward: -44, Average Reward per Agent: -8.80\n",
      "Episode 67/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 68/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 69/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 70/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 71/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 72/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 73/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 74/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 75/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 76/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 77/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 78/1000, Total Reward: -67, Average Reward per Agent: -13.40\n",
      "Episode 79/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 80/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 81/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 82/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 83/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 84/1000, Total Reward: -67, Average Reward per Agent: -13.40\n",
      "Episode 85/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 86/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 87/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 88/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 89/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 90/1000, Total Reward: -47, Average Reward per Agent: -9.40\n",
      "Episode 91/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 92/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 93/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 94/1000, Total Reward: -42, Average Reward per Agent: -8.40\n",
      "Episode 95/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 96/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 97/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 98/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 99/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 100/1000, Total Reward: -48, Average Reward per Agent: -9.60\n",
      "Episode 101/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 102/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 103/1000, Total Reward: -109, Average Reward per Agent: -21.80\n",
      "Episode 104/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 105/1000, Total Reward: -67, Average Reward per Agent: -13.40\n",
      "Episode 106/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 107/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 108/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 109/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 110/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 111/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 112/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 113/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 114/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 115/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 116/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 117/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 118/1000, Total Reward: -63, Average Reward per Agent: -12.60\n",
      "Episode 119/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 120/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 121/1000, Total Reward: -25, Average Reward per Agent: -5.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 122/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 123/1000, Total Reward: -67, Average Reward per Agent: -13.40\n",
      "Episode 124/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 125/1000, Total Reward: -42, Average Reward per Agent: -8.40\n",
      "Episode 126/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 127/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 128/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 129/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 130/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 131/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 132/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 133/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 134/1000, Total Reward: -39, Average Reward per Agent: -7.80\n",
      "Episode 135/1000, Total Reward: -63, Average Reward per Agent: -12.60\n",
      "Episode 136/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 137/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 138/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 139/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 140/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 141/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 142/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 143/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 144/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 145/1000, Total Reward: -63, Average Reward per Agent: -12.60\n",
      "Episode 146/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 147/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 148/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 149/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 150/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 151/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 152/1000, Total Reward: -42, Average Reward per Agent: -8.40\n",
      "Episode 153/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 154/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 155/1000, Total Reward: -101, Average Reward per Agent: -20.20\n",
      "Episode 156/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 157/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 158/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 159/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 160/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 161/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 162/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 163/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 164/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 165/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 166/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 167/1000, Total Reward: -65, Average Reward per Agent: -13.00\n",
      "Episode 168/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 169/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 170/1000, Total Reward: -10, Average Reward per Agent: -2.00\n",
      "Episode 171/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 172/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 173/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 174/1000, Total Reward: -26, Average Reward per Agent: -5.20\n",
      "Episode 175/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 176/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 177/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 178/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 179/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 180/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 181/1000, Total Reward: -126, Average Reward per Agent: -25.20\n",
      "Episode 182/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 183/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 184/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 185/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 186/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 187/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 188/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 189/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 190/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 191/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 192/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 193/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 194/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 195/1000, Total Reward: -107, Average Reward per Agent: -21.40\n",
      "Episode 196/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 197/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 198/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 199/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 200/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 201/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 202/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 203/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 204/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 205/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 206/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 207/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 208/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 209/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 210/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 211/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 212/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 213/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 214/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 215/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 216/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 217/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 218/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 219/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 220/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 221/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 222/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 223/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 224/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 225/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 226/1000, Total Reward: -67, Average Reward per Agent: -13.40\n",
      "Episode 227/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 228/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 229/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 230/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 231/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 232/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 233/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 234/1000, Total Reward: -10, Average Reward per Agent: -2.00\n",
      "Episode 235/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 236/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 237/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 238/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 239/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 240/1000, Total Reward: -42, Average Reward per Agent: -8.40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 241/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 242/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 243/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 244/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 245/1000, Total Reward: -65, Average Reward per Agent: -13.00\n",
      "Episode 246/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 247/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 248/1000, Total Reward: -10, Average Reward per Agent: -2.00\n",
      "Episode 249/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 250/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 251/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 252/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 253/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 254/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 255/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 256/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 257/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 258/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 259/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 260/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 261/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 262/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 263/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 264/1000, Total Reward: -42, Average Reward per Agent: -8.40\n",
      "Episode 265/1000, Total Reward: -67, Average Reward per Agent: -13.40\n",
      "Episode 266/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 267/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 268/1000, Total Reward: -10, Average Reward per Agent: -2.00\n",
      "Episode 269/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 270/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 271/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 272/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 273/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 274/1000, Total Reward: -10, Average Reward per Agent: -2.00\n",
      "Episode 275/1000, Total Reward: -26, Average Reward per Agent: -5.20\n",
      "Episode 276/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 277/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 278/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 279/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 280/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 281/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 282/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 283/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 284/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 285/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 286/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 287/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 288/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 289/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 290/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 291/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 292/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 293/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 294/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 295/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 296/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 297/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 298/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 299/1000, Total Reward: -781, Average Reward per Agent: -156.20\n",
      "Episode 300/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 301/1000, Total Reward: -67, Average Reward per Agent: -13.40\n",
      "Episode 302/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 303/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 304/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 305/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 306/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 307/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 308/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 309/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 310/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 311/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 312/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 313/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 314/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 315/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 316/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 317/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 318/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 319/1000, Total Reward: -42, Average Reward per Agent: -8.40\n",
      "Episode 320/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 321/1000, Total Reward: -10, Average Reward per Agent: -2.00\n",
      "Episode 322/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 323/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 324/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 325/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 326/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 327/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 328/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 329/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 330/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 331/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 332/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 333/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 334/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 335/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 336/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 337/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 338/1000, Total Reward: -47, Average Reward per Agent: -9.40\n",
      "Episode 339/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 340/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 341/1000, Total Reward: -88, Average Reward per Agent: -17.60\n",
      "Episode 342/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 343/1000, Total Reward: -89, Average Reward per Agent: -17.80\n",
      "Episode 344/1000, Total Reward: -48, Average Reward per Agent: -9.60\n",
      "Episode 345/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 346/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 347/1000, Total Reward: -48, Average Reward per Agent: -9.60\n",
      "Episode 348/1000, Total Reward: -697, Average Reward per Agent: -139.40\n",
      "Episode 349/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 350/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 351/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 352/1000, Total Reward: -42, Average Reward per Agent: -8.40\n",
      "Episode 353/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 354/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 355/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 356/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 357/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 358/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 359/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 360/1000, Total Reward: -25, Average Reward per Agent: -5.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 361/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 362/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 363/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 364/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 365/1000, Total Reward: -89, Average Reward per Agent: -17.80\n",
      "Episode 366/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 367/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 368/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 369/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 370/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 371/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 372/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 373/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 374/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 375/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 376/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 377/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 378/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 379/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 380/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 381/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 382/1000, Total Reward: -67, Average Reward per Agent: -13.40\n",
      "Episode 383/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 384/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 385/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 386/1000, Total Reward: -60, Average Reward per Agent: -12.00\n",
      "Episode 387/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 388/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 389/1000, Total Reward: -67, Average Reward per Agent: -13.40\n",
      "Episode 390/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 391/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 392/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 393/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 394/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 395/1000, Total Reward: -42, Average Reward per Agent: -8.40\n",
      "Episode 396/1000, Total Reward: -42, Average Reward per Agent: -8.40\n",
      "Episode 397/1000, Total Reward: -44, Average Reward per Agent: -8.80\n",
      "Episode 398/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 399/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 400/1000, Total Reward: -67, Average Reward per Agent: -13.40\n",
      "Episode 401/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 402/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 403/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 404/1000, Total Reward: -42, Average Reward per Agent: -8.40\n",
      "Episode 405/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 406/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 407/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 408/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 409/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 410/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 411/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 412/1000, Total Reward: -42, Average Reward per Agent: -8.40\n",
      "Episode 413/1000, Total Reward: -42, Average Reward per Agent: -8.40\n",
      "Episode 414/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 415/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 416/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 417/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 418/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 419/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 420/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 421/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 422/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 423/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 424/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 425/1000, Total Reward: -42, Average Reward per Agent: -8.40\n",
      "Episode 426/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 427/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 428/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 429/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 430/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 431/1000, Total Reward: -47, Average Reward per Agent: -9.40\n",
      "Episode 432/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 433/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 434/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 435/1000, Total Reward: -44, Average Reward per Agent: -8.80\n",
      "Episode 436/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 437/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 438/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 439/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 440/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 441/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 442/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 443/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 444/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 445/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 446/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 447/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 448/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 449/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 450/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 451/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 452/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 453/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 454/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 455/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 456/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 457/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 458/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 459/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 460/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 461/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 462/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 463/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 464/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 465/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 466/1000, Total Reward: -47, Average Reward per Agent: -9.40\n",
      "Episode 467/1000, Total Reward: -67, Average Reward per Agent: -13.40\n",
      "Episode 468/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 469/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 470/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 471/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 472/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 473/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 474/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 475/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 476/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 477/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 478/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 479/1000, Total Reward: -42, Average Reward per Agent: -8.40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 480/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 481/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 482/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 483/1000, Total Reward: -42, Average Reward per Agent: -8.40\n",
      "Episode 484/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 485/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 486/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 487/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 488/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 489/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 490/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 491/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 492/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 493/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 494/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 495/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 496/1000, Total Reward: -90, Average Reward per Agent: -18.00\n",
      "Episode 497/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 498/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 499/1000, Total Reward: -10, Average Reward per Agent: -2.00\n",
      "Episode 500/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 501/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 502/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 503/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 504/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 505/1000, Total Reward: -42, Average Reward per Agent: -8.40\n",
      "Episode 506/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 507/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 508/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 509/1000, Total Reward: -10, Average Reward per Agent: -2.00\n",
      "Episode 510/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 511/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 512/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 513/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 514/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 515/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 516/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 517/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 518/1000, Total Reward: -1882, Average Reward per Agent: -376.40\n",
      "Episode 519/1000, Total Reward: -63, Average Reward per Agent: -12.60\n",
      "Episode 520/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 521/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 522/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 523/1000, Total Reward: -466, Average Reward per Agent: -93.20\n",
      "Episode 524/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 525/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 526/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 527/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 528/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 529/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 530/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 531/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 532/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 533/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 534/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 535/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 536/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 537/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 538/1000, Total Reward: -42, Average Reward per Agent: -8.40\n",
      "Episode 539/1000, Total Reward: -67, Average Reward per Agent: -13.40\n",
      "Episode 540/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 541/1000, Total Reward: -61, Average Reward per Agent: -12.20\n",
      "Episode 542/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 543/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 544/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 545/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 546/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 547/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 548/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 549/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 550/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 551/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 552/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 553/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 554/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 555/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 556/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 557/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 558/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 559/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 560/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 561/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 562/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 563/1000, Total Reward: -10, Average Reward per Agent: -2.00\n",
      "Episode 564/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 565/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 566/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 567/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 568/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 569/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 570/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 571/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 572/1000, Total Reward: -64, Average Reward per Agent: -12.80\n",
      "Episode 573/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 574/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 575/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 576/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 577/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 578/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 579/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 580/1000, Total Reward: -10, Average Reward per Agent: -2.00\n",
      "Episode 581/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 582/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 583/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 584/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 585/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 586/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 587/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 588/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 589/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 590/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 591/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 592/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 593/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 594/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 595/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 596/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 597/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 598/1000, Total Reward: -46, Average Reward per Agent: -9.20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 599/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 600/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 601/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 602/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 603/1000, Total Reward: -42, Average Reward per Agent: -8.40\n",
      "Episode 604/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 605/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 606/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 607/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 608/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 609/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 610/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 611/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 612/1000, Total Reward: -10, Average Reward per Agent: -2.00\n",
      "Episode 613/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 614/1000, Total Reward: -67, Average Reward per Agent: -13.40\n",
      "Episode 615/1000, Total Reward: -44, Average Reward per Agent: -8.80\n",
      "Episode 616/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 617/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 618/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 619/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 620/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 621/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 622/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 623/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 624/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 625/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 626/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 627/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 628/1000, Total Reward: -42, Average Reward per Agent: -8.40\n",
      "Episode 629/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 630/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 631/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 632/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 633/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 634/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 635/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 636/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 637/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 638/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 639/1000, Total Reward: -2083, Average Reward per Agent: -416.60\n",
      "Episode 640/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 641/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 642/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 643/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 644/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 645/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 646/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 647/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 648/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 649/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 650/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 651/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 652/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 653/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 654/1000, Total Reward: -67, Average Reward per Agent: -13.40\n",
      "Episode 655/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 656/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 657/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 658/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 659/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 660/1000, Total Reward: -42, Average Reward per Agent: -8.40\n",
      "Episode 661/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 662/1000, Total Reward: -68, Average Reward per Agent: -13.60\n",
      "Episode 663/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 664/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 665/1000, Total Reward: -63, Average Reward per Agent: -12.60\n",
      "Episode 666/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 667/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 668/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 669/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 670/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 671/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 672/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 673/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 674/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 675/1000, Total Reward: -10, Average Reward per Agent: -2.00\n",
      "Episode 676/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 677/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 678/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 679/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 680/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 681/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 682/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 683/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 684/1000, Total Reward: -10, Average Reward per Agent: -2.00\n",
      "Episode 685/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 686/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 687/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 688/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 689/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 690/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 691/1000, Total Reward: -42, Average Reward per Agent: -8.40\n",
      "Episode 692/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 693/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 694/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 695/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 696/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 697/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 698/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 699/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 700/1000, Total Reward: -67, Average Reward per Agent: -13.40\n",
      "Episode 701/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 702/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 703/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 704/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 705/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 706/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 707/1000, Total Reward: -67, Average Reward per Agent: -13.40\n",
      "Episode 708/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 709/1000, Total Reward: -42, Average Reward per Agent: -8.40\n",
      "Episode 710/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 711/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 712/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 713/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 714/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 715/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 716/1000, Total Reward: -42, Average Reward per Agent: -8.40\n",
      "Episode 717/1000, Total Reward: -25, Average Reward per Agent: -5.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 718/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 719/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 720/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 721/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 722/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 723/1000, Total Reward: -10, Average Reward per Agent: -2.00\n",
      "Episode 724/1000, Total Reward: -67, Average Reward per Agent: -13.40\n",
      "Episode 725/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 726/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 727/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 728/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 729/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 730/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 731/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 732/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 733/1000, Total Reward: -42, Average Reward per Agent: -8.40\n",
      "Episode 734/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 735/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 736/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 737/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 738/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 739/1000, Total Reward: -67, Average Reward per Agent: -13.40\n",
      "Episode 740/1000, Total Reward: -67, Average Reward per Agent: -13.40\n",
      "Episode 741/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 742/1000, Total Reward: -67, Average Reward per Agent: -13.40\n",
      "Episode 743/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 744/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 745/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 746/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 747/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 748/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 749/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 750/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 751/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 752/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 753/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 754/1000, Total Reward: -38, Average Reward per Agent: -7.60\n",
      "Episode 755/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 756/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 757/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 758/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 759/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 760/1000, Total Reward: -67, Average Reward per Agent: -13.40\n",
      "Episode 761/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 762/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 763/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 764/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 765/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 766/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 767/1000, Total Reward: -42, Average Reward per Agent: -8.40\n",
      "Episode 768/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 769/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 770/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 771/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 772/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 773/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 774/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 775/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 776/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 777/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 778/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 779/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 780/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 781/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 782/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 783/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 784/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 785/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 786/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 787/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 788/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 789/1000, Total Reward: -67, Average Reward per Agent: -13.40\n",
      "Episode 790/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 791/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 792/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 793/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 794/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 795/1000, Total Reward: -48, Average Reward per Agent: -9.60\n",
      "Episode 796/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 797/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 798/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 799/1000, Total Reward: -67, Average Reward per Agent: -13.40\n",
      "Episode 800/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 801/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 802/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 803/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 804/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 805/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 806/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 807/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 808/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 809/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 810/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 811/1000, Total Reward: -42, Average Reward per Agent: -8.40\n",
      "Episode 812/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 813/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 814/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 815/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 816/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 817/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 818/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 819/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 820/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 821/1000, Total Reward: -67, Average Reward per Agent: -13.40\n",
      "Episode 822/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 823/1000, Total Reward: -63, Average Reward per Agent: -12.60\n",
      "Episode 824/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 825/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 826/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 827/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 828/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 829/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 830/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 831/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 832/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 833/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 834/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 835/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 836/1000, Total Reward: -25, Average Reward per Agent: -5.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 837/1000, Total Reward: -63, Average Reward per Agent: -12.60\n",
      "Episode 838/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 839/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 840/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 841/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 842/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 843/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 844/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 845/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 846/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 847/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 848/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 849/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 850/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 851/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 852/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 853/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 854/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 855/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 856/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 857/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 858/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 859/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 860/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 861/1000, Total Reward: -42, Average Reward per Agent: -8.40\n",
      "Episode 862/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 863/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 864/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 865/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 866/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 867/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 868/1000, Total Reward: -67, Average Reward per Agent: -13.40\n",
      "Episode 869/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 870/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 871/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 872/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 873/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 874/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 875/1000, Total Reward: -31, Average Reward per Agent: -6.20\n",
      "Episode 876/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 877/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 878/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 879/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 880/1000, Total Reward: -44, Average Reward per Agent: -8.80\n",
      "Episode 881/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 882/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 883/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 884/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 885/1000, Total Reward: -42, Average Reward per Agent: -8.40\n",
      "Episode 886/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 887/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 888/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 889/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 890/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 891/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 892/1000, Total Reward: -1701, Average Reward per Agent: -340.20\n",
      "Episode 893/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 894/1000, Total Reward: -42, Average Reward per Agent: -8.40\n",
      "Episode 895/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 896/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 897/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 898/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 899/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 900/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 901/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 902/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 903/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 904/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 905/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 906/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 907/1000, Total Reward: -42, Average Reward per Agent: -8.40\n",
      "Episode 908/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 909/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 910/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 911/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 912/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 913/1000, Total Reward: -26, Average Reward per Agent: -5.20\n",
      "Episode 914/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 915/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 916/1000, Total Reward: -10, Average Reward per Agent: -2.00\n",
      "Episode 917/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 918/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 919/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 920/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 921/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 922/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 923/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 924/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 925/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 926/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 927/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 928/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 929/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 930/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 931/1000, Total Reward: -10, Average Reward per Agent: -2.00\n",
      "Episode 932/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 933/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 934/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 935/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 936/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 937/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 938/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 939/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 940/1000, Total Reward: -90, Average Reward per Agent: -18.00\n",
      "Episode 941/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 942/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 943/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 944/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 945/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 946/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 947/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 948/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 949/1000, Total Reward: -63, Average Reward per Agent: -12.60\n",
      "Episode 950/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 951/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 952/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 953/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 954/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 955/1000, Total Reward: -25, Average Reward per Agent: -5.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 956/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 957/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 958/1000, Total Reward: -26, Average Reward per Agent: -5.20\n",
      "Episode 959/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 960/1000, Total Reward: -10, Average Reward per Agent: -2.00\n",
      "Episode 961/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 962/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 963/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 964/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 965/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 966/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 967/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 968/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 969/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 970/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 971/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 972/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 973/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 974/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 975/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 976/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 977/1000, Total Reward: -63, Average Reward per Agent: -12.60\n",
      "Episode 978/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 979/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 980/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 981/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 982/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 983/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 984/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 985/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 986/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 987/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 988/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 989/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 990/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 991/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 992/1000, Total Reward: -42, Average Reward per Agent: -8.40\n",
      "Episode 993/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 994/1000, Total Reward: -46, Average Reward per Agent: -9.20\n",
      "Episode 995/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 996/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 997/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 998/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 999/1000, Total Reward: -25, Average Reward per Agent: -5.00\n",
      "Episode 1000/1000, Total Reward: -10, Average Reward per Agent: -2.00\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = RailEnvironment(grid_size=(10, 10), n_agents=5)  # Environment with multiple agents\n",
    "    agents = [\n",
    "        DQLAgent(state_size=env.grid.size, action_size=5) for _ in range(env.n_agents)\n",
    "    ]  # One DQLAgent for each agent\n",
    "    episodes = 1000\n",
    "    batch_size = 32\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()  # Reset environment\n",
    "        states = [state.flatten()] * env.n_agents  # Initial states for all agents\n",
    "        done = False\n",
    "        total_rewards = [0] * env.n_agents  # Track total reward for each agent\n",
    "\n",
    "        while not done:\n",
    "            # Agents select actions independently\n",
    "            actions = [agent.act(state.flatten()) for agent, state in zip(agents, states)]\n",
    "\n",
    "            # Perform actions in the environment\n",
    "            next_state, rewards, done = env.step(actions)\n",
    "\n",
    "            # Update each agent's experience and track rewards\n",
    "            next_states = [next_state.flatten()] * env.n_agents\n",
    "            for i, agent in enumerate(agents):\n",
    "                reward = rewards[i]\n",
    "                agent.remember(states[i], actions[i], reward, next_states[i], done)\n",
    "                total_rewards[i] += reward\n",
    "\n",
    "            states = next_states  # Update states for the next step\n",
    "\n",
    "        # Replay and train each agent\n",
    "        for agent in agents:\n",
    "            agent.replay(batch_size)\n",
    "\n",
    "        # Logging the results for this episode\n",
    "        total_reward = sum(total_rewards)\n",
    "        print(f\"Episode {episode + 1}/{episodes}, Total Reward: {total_reward}, Average Reward per Agent: {total_reward / env.n_agents:.2f}\")\n",
    "\n",
    "        # Optionally, you can adjust epsilon decay for each agent if needed\n",
    "        for agent in agents:\n",
    "            if hasattr(agent, 'epsilon') and hasattr(agent, 'epsilon_min'):\n",
    "                agent.epsilon = max(agent.epsilon_min, agent.epsilon * 0.995)  # Decay epsilon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff53a03",
   "metadata": {},
   "source": [
    "# Second Deliverable: Simulation and evaluation of model based on historical performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f18e35a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import heapq\n",
    "\n",
    "class RealWorldRailEnvironment:\n",
    "    def __init__(self, grid_size=(100, 100), n_agents=3, station_coords=None):\n",
    "        self.grid_size = grid_size\n",
    "        self.n_agents = n_agents\n",
    "        self.grid = np.full(grid_size, -1)  # Initialize grid with non-rail area (-1)\n",
    "        self.agents = []\n",
    "        self.station_map = station_coords or {}  # Map stations to grid coordinates\n",
    "        self._initialize_environment()\n",
    "\n",
    "    def _initialize_environment(self):\n",
    "        # Mark stations\n",
    "        for station, location in self.station_map.items():\n",
    "            self.grid[location] = 0  # Stations are railway nodes\n",
    "\n",
    "        # Connect stations with realistic routes\n",
    "        station_locations = list(self.station_map.values())\n",
    "        for i in range(len(station_locations) - 1):\n",
    "            self._draw_route(station_locations[i], station_locations[i + 1])\n",
    "\n",
    "        # Randomly assign agents to simulate trains\n",
    "        for i in range(self.n_agents):\n",
    "            start_station = random.choice(list(self.station_map.keys()))\n",
    "            target_station = random.choice(list(self.station_map.keys()))\n",
    "            while start_station == target_station:\n",
    "                target_station = random.choice(list(self.station_map.keys()))\n",
    "\n",
    "            self.agents.append({\n",
    "                \"id\": i,\n",
    "                \"start\": self.station_map[start_station],\n",
    "                \"target\": self.station_map[target_station],\n",
    "                \"position\": self.station_map[start_station],\n",
    "                \"done\": False,\n",
    "                \"reward\": 0,\n",
    "                \"fuel_consumption\": 0.0  # Track fuel consumption\n",
    "            })\n",
    "            self.grid[self.station_map[start_station]] = 1  # Mark starting positions\n",
    "\n",
    "    def _draw_route(self, start, end):\n",
    "        \"\"\"Connect two stations using A* pathfinding.\"\"\"\n",
    "        path = self._a_star(start, end)\n",
    "        for x, y in path:\n",
    "            if self.grid[x, y] == -1:  # Avoid overwriting stations\n",
    "                self.grid[x, y] = 0  # Railway track\n",
    "\n",
    "    def _a_star(self, start, end):\n",
    "        \"\"\"A* pathfinding algorithm for realistic routes.\"\"\"\n",
    "        def heuristic(a, b):\n",
    "            return abs(a[0] - b[0]) + abs(a[1] - b[1])  # Manhattan distance\n",
    "\n",
    "        open_set = []\n",
    "        heapq.heappush(open_set, (0, start))\n",
    "        came_from = {}\n",
    "        g_score = {start: 0}\n",
    "        f_score = {start: heuristic(start, end)}\n",
    "\n",
    "        while open_set:\n",
    "            _, current = heapq.heappop(open_set)\n",
    "\n",
    "            if current == end:\n",
    "                return self._reconstruct_path(came_from, current)\n",
    "\n",
    "            for neighbor in self._get_neighbors(current):\n",
    "                tentative_g_score = g_score[current] + 1\n",
    "                if neighbor not in g_score or tentative_g_score < g_score[neighbor]:\n",
    "                    came_from[neighbor] = current\n",
    "                    g_score[neighbor] = tentative_g_score\n",
    "                    f_score[neighbor] = g_score[neighbor] + heuristic(neighbor, end)\n",
    "                    if neighbor not in [item[1] for item in open_set]:\n",
    "                        heapq.heappush(open_set, (f_score[neighbor], neighbor))\n",
    "\n",
    "        return []  # No path found\n",
    "\n",
    "    def _get_neighbors(self, position):\n",
    "        \"\"\"Get valid neighbors for A* algorithm.\"\"\"\n",
    "        directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "        neighbors = []\n",
    "        for dx, dy in directions:\n",
    "            x, y = position[0] + dx, position[1] + dy\n",
    "            if 0 <= x < self.grid_size[0] and 0 <= y < self.grid_size[1] and self.grid[x, y] != 1:\n",
    "                neighbors.append((x, y))\n",
    "        return neighbors\n",
    "\n",
    "    def _reconstruct_path(self, came_from, current):\n",
    "        \"\"\"Reconstruct the path from A*.\"\"\"\n",
    "        path = [current]\n",
    "        while current in came_from:\n",
    "            current = came_from[current]\n",
    "            path.append(current)\n",
    "        path.reverse()\n",
    "        return path\n",
    "\n",
    "    def step(self, actions):\n",
    "        rewards = {}\n",
    "        done = True\n",
    "        for agent, action in zip(self.agents, actions):\n",
    "            if agent[\"done\"]:\n",
    "                rewards[agent[\"id\"]] = 0\n",
    "                continue\n",
    "\n",
    "            new_position = self._move(agent[\"position\"], action)\n",
    "            if not self._is_valid(new_position):\n",
    "                rewards[agent[\"id\"]] = -5  # Invalid move penalty\n",
    "                continue\n",
    "\n",
    "            # Calculate distance traveled for fuel consumption\n",
    "            distance = self._calculate_distance(agent[\"position\"], new_position)\n",
    "            agent[\"fuel_consumption\"] += distance * 0.5  # Example: 0.5L/km fuel consumption\n",
    "\n",
    "            # Update position and check rewards\n",
    "            self.grid[agent[\"position\"]] = 0  # Free current position\n",
    "            agent[\"position\"] = new_position\n",
    "            self.grid[new_position] = 1  # Occupy new position\n",
    "\n",
    "            if new_position == agent[\"target\"]:\n",
    "                agent[\"done\"] = True\n",
    "                rewards[agent[\"id\"]] = 10  # Target reached reward\n",
    "            else:\n",
    "                rewards[agent[\"id\"]] = -1  # Step penalty\n",
    "\n",
    "            done = done and agent[\"done\"]\n",
    "\n",
    "        return self.grid, rewards, done\n",
    "\n",
    "    def _move(self, position, action):\n",
    "        \"\"\"Move based on the action.\"\"\"\n",
    "        moves = {\n",
    "            0: (-1, 0),  # Up\n",
    "            1: (1, 0),   # Down\n",
    "            2: (0, -1),  # Left\n",
    "            3: (0, 1),   # Right\n",
    "            4: (0, 0)    # Wait\n",
    "        }\n",
    "        return (position[0] + moves[action][0], position[1] + moves[action][1])\n",
    "\n",
    "    def _is_valid(self, position):\n",
    "        \"\"\"Check if the position is valid.\"\"\"\n",
    "        return (0 <= position[0] < self.grid_size[0] and\n",
    "                0 <= position[1] < self.grid_size[1] and\n",
    "                self.grid[position] >= 0 and\n",
    "                self.grid[position] != 1)\n",
    "\n",
    "    def _calculate_distance(self, pos1, pos2):\n",
    "        \"\"\"Calculate Euclidean distance.\"\"\"\n",
    "        return math.sqrt((pos2[0] - pos1[0])**2 + (pos2[1] - pos1[1])**2)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment.\"\"\"\n",
    "        self.__init__(self.grid_size, self.n_agents, self.station_map)\n",
    "        return self.grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b07788f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_distance_comparison(env, agents, episodes, batch_size, real_world_distances):\n",
    "    best_simulated_distances = [float(\"inf\")] * env.n_agents\n",
    "    best_results = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        states = [state.flatten()] * env.n_agents\n",
    "        done = False\n",
    "        total_distance = [0] * env.n_agents\n",
    "\n",
    "        while not done:\n",
    "            # Agents select actions independently\n",
    "            actions = [agent.act(state.flatten()) for agent, state in zip(agents, states)]\n",
    "            next_state, rewards, done = env.step(actions)\n",
    "            next_states = [next_state.flatten()] * env.n_agents\n",
    "\n",
    "            # Calculate distances for each agent\n",
    "            for i in range(env.n_agents):\n",
    "                total_distance[i] += env._calculate_distance(\n",
    "                    env.agents[i][\"position\"], next_state.flatten()\n",
    "                )\n",
    "\n",
    "            states = next_states\n",
    "\n",
    "        # Store the best distance for each agent\n",
    "        for i in range(env.n_agents):\n",
    "            existing_result = next((result for result in best_results if result[\"agent_id\"] == i), None)\n",
    "            if existing_result:\n",
    "                # Update if the current distance is better\n",
    "                if total_distance[i] < existing_result[\"simulated_distance\"]:\n",
    "                    existing_result[\"simulated_distance\"] = total_distance[i]\n",
    "            else:\n",
    "            # Add a new result for the agent\n",
    "                best_results.append(\n",
    "                {\n",
    "                    \"agent_id\": i,\n",
    "                    \"real_world_distance\": real_world_distances[i],\n",
    "                    \"simulated_distance\": total_distance[i],\n",
    "                }\n",
    "                )\n",
    "\n",
    "\n",
    "        # Replay for training\n",
    "        for agent in agents:\n",
    "            agent.replay(batch_size)\n",
    "\n",
    "    # Output best results\n",
    "    print(\"\\n--- Distance Comparison Results ---\")\n",
    "    for result in best_results:\n",
    "        print(\n",
    "            f\"Agent {result['agent_id']}:\\n\"\n",
    "            f\"- Real-World Distance: {result['real_world_distance']} km\\n\"\n",
    "            f\"- Best Simulated Distance: {result['simulated_distance']} km\\n\"\n",
    "            f\"- Improvement: {100 * (result['real_world_distance'] - result['simulated_distance']) / result['real_world_distance']:.2f}%\\n\"\n",
    "        )\n",
    "        \n",
    "    real_world_distances = [result[\"real_world_distance\"] for result in best_results]\n",
    "    best_simulated_distances = [result[\"simulated_distance\"] for result in best_results]\n",
    "    visualize_comparison(real_world_distances, best_simulated_distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70c5f9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Distance Comparison Results ---\n",
      "Agent 0:\n",
      "- Real-World Distance: 1400 km\n",
      "- Best Simulated Distance: 15.556349186104045 km\n",
      "- Improvement: 98.89%\n",
      "\n",
      "Agent 1:\n",
      "- Real-World Distance: 1380 km\n",
      "- Best Simulated Distance: 15.556349186104045 km\n",
      "- Improvement: 98.87%\n",
      "\n",
      "Agent 2:\n",
      "- Real-World Distance: 1420 km\n",
      "- Best Simulated Distance: 15.556349186104045 km\n",
      "- Improvement: 98.90%\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example real-world distances for a route (in km)\n",
    "    real_world_distances = [1400, 1380, 1420]  # Example for 3 trains\n",
    "    \n",
    "    # Example station data\n",
    "    station_coords = {\n",
    "        \"Delhi\": (10, 10),\n",
    "        \"Agra\": (20, 15),\n",
    "        \"Gwalior\": (30, 20),\n",
    "        \"Jhansi\": (40, 30),\n",
    "        \"Bhopal\": (60, 50),\n",
    "        \"Mumbai\": (90, 90),\n",
    "    }\n",
    "\n",
    "    # Initialize the environment and agents\n",
    "    env = RealWorldRailEnvironment(grid_size=(100, 100), n_agents=len(real_world_distances), station_coords=station_coords)\n",
    "    agents = [DQLAgent(state_size=env.grid.size, action_size=5) for _ in range(env.n_agents)]\n",
    "    \n",
    "    # Run the simulation and compare results\n",
    "    simulate_distance_comparison(env, agents, episodes=1000, batch_size=32, real_world_distances=real_world_distances)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89c21486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_comparison(real_world_distances, simulated_distances):\n",
    "    agents = range(len(real_world_distances))\n",
    "    plt.bar(agents, real_world_distances, label=\"Real-World Distance\", color=\"red\", alpha=0.6)\n",
    "    plt.bar(agents, simulated_distances, label=\"Simulated Distance\", color=\"blue\", alpha=0.6)\n",
    "    plt.xlabel(\"Agent ID\")\n",
    "    plt.ylabel(\"Distance (km)\")\n",
    "    plt.title(\"Distance Comparison\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
